# Lecture 13 : Attention

# Attention의 목적

---

1. **하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생**

2. **RNN의 고질적인 문제인 기울기 소실(vanishing gradient) 문제가 존재**

기존 RNN 모델에는 두 가지 문제가 있다.

어텐션(attention) : 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위한 등장한 기법

어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고하는 방법이다.

단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 참고한다.

# Seq2Seq 구조

---

Encoder에서 input sequence에 대한 정보를 압축해서 저장한다.

그리고 decoder의 initial hidden state가 될 $s_0$, 모든 시점에서 decoder에 입력으로 들어갈 context vector $c$를 output으로 내보낸다.

보통 $c$를 final hidden state라고 본다. $s_0$는 fully connected layer에서 예측되었다고 가정한다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled.png)

전체 forward pass 구조는 다음과 같다.

context vector는 encoded seq에서 decoded seq으로 정보를 전달하는데 사용한다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%201.png)

하지만, 하나의 c에 긴 글을 담을 수 없다

그래서 모든 decoder step마다 c를 새로 계산한다. 때마다 다른 input sequence에 focus 하도록 만든다.

이를 Attention Mechanism이라고 한다.

# Seq2Seq에 Attention을 적용한 구조

---

자세한 계산식은 그림 참고

parameter update에 관해서, 어떤 부분이 중요하다고 넷에 직접 전달하는게 아니라(attention weight 조작 하는 것 없다는 의미이다.), 오차역전파로 자동으로 해결된다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%202.png)

Alignment Funtion을 이용해서 Attention을 구현한다. $f_{ATT}$로 표현한다. 이 함수는 FC Layer로 표현된다.

Alignment Funtion은 current hidden state of encoder, 각각의 hidden state of decoder 두 가지를 입력으로 받는다.

출력으로는 현재 decoder의 hidden state 기준으로, 각 encoder의 hidden state에 대해서 얼마나 집중해야하는 지 분포를 내보낸다. 이 분포를 attention weights라고 한다.

이걸로 가중합을 구해서 Context Vector를 구한다

이걸 반복한다.

s1으로 다시 c2를 만들고,,

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%203.png)

s2로 다시 c3를 만든다...

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%204.png)

특히 NLP에서, RNN의 Output은 input의 어딘가와 연관되어있다. 이런 관점으로, decoder가 각 시점마다 encoder의 다른 부분에 집중하도록 새롭게  Context Vector를 생성해야한다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%205.png)

Attention Weight를 구해서 모델의 다음 output을 결정한다.

Attention Weight는 모델이 weigth를 구하는 것이지, 우리가 weight를 전달하는 것이 아니다.

다음 그림을 보자.

대체로 대각선 형태로 확률 분포가 형성되어있는 것을 볼 수 있다. 즉, 원문과 번역본의 순서가 잘 맞는다는 얘기다.

물론 순서가 뒤바뀐다고 문제가 생기지는 않는다. 이는 초록색 영역을 보면 알 수 있다.

함께 쓰이는 동사는 분포가 다소 뭉쳐있는 것을 알 수 있다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%206.png)

입력이 꼭 시퀀스일 필요는 없다. 어텐션 메커니즘이 그걸 신경쓰지는 않은니깐

그래서 이미지에 사용한다.

# Image Captioning with RNNS and Attention

---

initial hidden state  $s_0$는 encoder로 부터 나온다.

직전의 attension seq2seq 모델처럼, alignment score를 구하고, 그걸 softmax해서 attenstion weight을 구한다.

그리고 attention weight과 feature vector(여기서는 hidden state 역할을 맡는다.)를 가중합 ㅐ서 $c_1$을 만든다.

공식은 아래 그림의 공식을 따른다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%207.png)

$c_t$는 vector, $h_{i,j}$는 vector, $e_{t,i,j}$는 scalar, $a_{t,i,j}$는 scalar 

 $e_{t,i,j}$는 vector $h_{i,j}$를 얼마나 강조할 지 정한다. $a_{t,i,j}$는 그걸 normalize한 것.

$c_t$는 $a_{t,i,j}$와 $h_{i,j}$를 가중합 한 것. 즉, 벡터이다.

그 다음도 같은 과정을 거친다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%208.png)

매번의 timestep마다, decoder는 다른 context vector를 사용한다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%209.png)

## Example

각 단어를 output하는 시점마다, weight가 적절히 다른 위치에 가있는 것을 알 수 있다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2010.png)

아래 그림의 경우, 본 강의에서는 이후에 다룬다.

설명하자면, 아예 attention을 하나의 포인트에 대해서만 주도록 하고, weighted recombination이 아니라 그냥 지정된 지점의 feature를 이용하도록 한 경우

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2011.png)

어쨌든 이런 Attention과 같은 방법이 실제 눈이 빠르게 전체 부분을 흝는 것과 비슷하다고 한다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2012.png)

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2013.png)

time step마다 output을 만들어내고 싶고, 그 time step마다 집중해야하는 부분이 다르다면, attention mechanism이 적절하다고 할 수 있겠다.

# Attention Layer

---

직전에서 보았던 것 그대로 코드로 옮겨보자.

query vector q를 넣는다. 이는 hidden state(이미지에서 나온 것)를 대체하는 역할.

Similarities 연산은 $f_{ATT}$이고, 이는 Q와 X를 비교하는 연산을 한다.

a는 이 결과를 softmax로 확률분포로 만든 것이고,

y는 a와 input X를 weight sum한 것이다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2014.png)

similiarity function을 dot product로 바꾼다.

이제 matrix multiplication으로 silimilarity를 계산할 수 있다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2015.png)

이를 발전 시켜서 scaled dot product를 쓰는 방법도 있다.

그러면 saturated region으로 softmax가 가서 gradient가 0이 되지 않게 할 수 있다.

또한 hidden vector의 dimension이 커짐에 따라 gradient가 exploding하는 문제도 방지할 수 있다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2016.png)

이번에는 query vector를 multiple하게 받는 것

그동안은 하나의 query vector를 받았으나, 이제는 여러 개를 받겠다는 뜻이다.

그리고 각각의 query vector에 대한 probability distribution을 얻는다.  

이제는 output도 여러 개가 나온다. 하나의 output이 하나의 query vector에 관한것

위의 연산을 모두 하나의 Matrix Multiplication($Y=AX$)으로 해결하자.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2017.png)

이번에는 input vector를 손본다.

앞서는 input vector를 query vector와의 similarity를 계산하는데 쓰고 output vectors를 계산하는데에도 썼었다.

이 두가지를 구분하기 위해서 Key matrix, value matrix로 구분한다. 둘다 learnable하다.

이 두가지 matrix를 input vector X에 곱하면 새로운 matrix Key/Value vector K, V가 등장한다.

이에 따라서 공식이 다음과 같이 변화한다. E = QK^T, Y = AV

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2018.png)

이제 전체적인 Layer 구조를 보자.

softmax를 vertical dimension에 가한다. → column이 x1, x2, x3에 대한 probability distribution이 된다.

다시 V를 계산해서 구하고, A의 column과 내적해서 Y를 구한다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2019.png)

이렇게 함으로 써 모든 query가 input과 결합하여 그 결과가 나올 수 있도록 할 수 있다.

### 왜 K, V를 구하는가?

input을 두가지로 나눠서 쓰면 flexibility 증가

예시를 들면,

query에 대한 걸 찾고 싶다. 이미 알고 있는 것을 제외한 다른 것(관련된 것)을 알고싶다.

유명한 건물 높이를 알고싶다고 하면, 몇 미터인지, 얼마나 높은지는 data 전체가 아니라 separate daata이다.

V, W로 나눠서 계산하면 이런 ‘관련 검색 결과’를 쉽게 구할 수 있다.

**이제 실제 예시를 돌아보자.**

# Self Attention Layer

---

하나의 vector set만을 받는다.

input된 vector set 내부의 vector끼리 비교하는 것이 목적이다.

이를 위해서 다른 learnable matrix $W_Q$를 추가한다.

사실 그냥 Query가 input에서 온 것 제외하면 다를 게 없다.

이 Layer가 non-Linear한 것을 기억하자.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2020.png)

만약 순서를 변경하면 어떻게 되나? 순서를 x3, x1, x2로 변경해봤다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2021.png)

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2022.png)

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2023.png)

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2024.png)

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2025.png)

그냥 모든 연산의 output이 내용물은 같고 순서만 다르게 나오는 것을 알 수 있다.

즉, attention은 vector끼리의 순서를 모른다! 이런 Layer는 네가 처음이야...

그런데 순서를 알아야하는 경우가 있을 수 있는데, 이를 위해서 positional encoding을 하기도 한다.

이를 통해서 시퀀스의 시작과 끝, 순서를 학습시킬 수 있다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2026.png)

# Masked Self-Attention Layer

---

**진짜 mask를 씌우는 것**

모든 information을 과거의 것만 쓰도록 만들고 싶다. 그리고 RNN에서 이런 방식의 forward pass에 영감을 받아서 만들어졌다.

앞선 default self attention layer의 경우, 모든 input vector를 사용할 수 있었다. 그래서 language 모델에서는 적합하지 않았다.

과거 sequence에 영향을 받는 language 모델에 적합하게 만들기 위해, 앞서 나오지 않은 vector에 관한 matrix의 일부를 가린다. -∞으로

그럼 softmax를 거쳐서 나온 부분이 0으로 나온다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2027.png)

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2028.png)

# Multihead Self-Attention Layer

---

H개의 Self-Attention Layer를 돌린다.

그리고 input을 h개로 균등히 나눠서 각 layer로 공급하고, 나온 output을 concatenate한다.

두 가지 Hyperparameter가 생긴다. Query의 dimenstion D_Q와 head 개수 H

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2029.png)

# Example : CNN with Self-Attention

---

Query와 Key는 서로 dot product한다.

output grid의 모든 위치가 input grid의 대응되는 위치에 영향을 받는다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2030.png)

# Three Ways of Processing Sequences

---

Attention은 RNN과 CNN의 문제를 모두 해결했다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2031.png)

시퀀스를 다룬다면 self attention만으로 모든 것을 해결할 수 있다는 뜻

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2032.png)

# Transformer

---

위의 논문에서 나온 것이 바로 이 Transformer

1. Self Attention
    
    output이 각각의 input에 의존적
    
    Multiple heads로 돌린다.
    
    Residual connection 만든다.
    
2. Layer Normalization
    
    optimization을 향상 시킨다.
    
    Conv Net에서의 batch normalization과 같은 기능이라고 생각.
    
    들어온 각각의 vector간의 연관성은 무시하고 각자 독립적으로 normalize
    
3. MLP
    
    각 vector 출력 별로 입력한다.
    
    Residual connection 만든다.
    
4. Layer Normalization

**# of input vectors = # of output vectors**

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2033.png)

이것 하나가 building block이 된다. 이걸로 큰 규모의 sequence model을 만든다.

vector끼리의 interaction은 오로지 self attention layer에서 발생한다.

paralyzable and scalable

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2034.png)

Transformer block으로 Transformer model을 만들 수 있다.

**Hyperparameter**

**depth of the model = # of blocks**

**query dimenstion**

**각 블록 내부에서 self attention의 head 개수**

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2035.png)

이게 잘 됐다고 한다. 비슷한 방식으로 전이학습 가능하다고 한다.

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2036.png)

Transformer를 이용해서 모델을 크게 만드는 결투의 참상

![Untitled](Lecture%2013%20Attention%20d4383a5e99164a05a1928d22b220f977/Untitled%2037.png)

# Reference

---

[1) 어텐션 메커니즘 (Attention Mechanism) - 딥 러닝을 이용한 자연어 처리 입문 (wikidocs.net)](https://wikidocs.net/22893)